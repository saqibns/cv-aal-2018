{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX 1050 Ti (CNMeM is disabled, cuDNN 5110)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, BatchNormalization, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "          rotation_range=40, width_shift_range=0.2,\n",
    "          height_shift_range=0.2, rescale=1./100,\n",
    "          shear_range=0.2, zoom_range=0.2,\n",
    "          horizontal_flip=True, fill_mode='nearest'\n",
    "           )\n",
    "\n",
    "validate_datagen = ImageDataGenerator(rescale=1./100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'Data/Heatmaps/Train-Simple-Float-Gauss/TrainFloatGaussian/'\n",
    "validation_data_dir = 'Data/Heatmaps/Train-Simple-Float-Gauss/TrainFloatGaussian-Val/'\n",
    "nb_train_samples = 1005 + 1035 + 1117\n",
    "nb_validation_samples = 112 + 115 + 124\n",
    "epochs = 100\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 256, 256)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3157 images belonging to 3 classes.\n",
      "Found 351 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    train_data_dir,\n",
    "                    target_size=(256, 256),\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode='categorical'\n",
    "                    )\n",
    "\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "                            validation_data_dir,\n",
    "                            target_size=(256, 256),\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode='categorical'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(filepath='Workshop-Paper/Vanilla-Gaussian/vanila-gauss-{epoch:03d}-{val_acc:.3f}', save_best_only=True, monitor='val_acc', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Workshop-Paper/Vanilla-Gaussian/old/vanila-gauss-last-iteration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0615 - acc: 0.4290Epoch 00001: val_acc improved from -inf to 0.48810, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-001-0.488\n",
      "197/197 [==============================] - 151s 767ms/step - loss: 1.0610 - acc: 0.4294 - val_loss: 1.0233 - val_acc: 0.4881\n",
      "Epoch 2/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0267 - acc: 0.4646Epoch 00002: val_acc did not improve\n",
      "197/197 [==============================] - 146s 743ms/step - loss: 1.0264 - acc: 0.4648 - val_loss: 1.0196 - val_acc: 0.4554\n",
      "Epoch 3/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0176 - acc: 0.4861Epoch 00003: val_acc improved from 0.48810 to 0.49107, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-003-0.491\n",
      "197/197 [==============================] - 147s 747ms/step - loss: 1.0176 - acc: 0.4859 - val_loss: 1.0199 - val_acc: 0.4911\n",
      "Epoch 4/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0006 - acc: 0.5024Epoch 00004: val_acc improved from 0.49107 to 0.50298, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-004-0.503\n",
      "197/197 [==============================] - 135s 686ms/step - loss: 1.0005 - acc: 0.5030 - val_loss: 1.0081 - val_acc: 0.5030\n",
      "Epoch 5/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0068 - acc: 0.4957Epoch 00005: val_acc did not improve\n",
      "197/197 [==============================] - 134s 682ms/step - loss: 1.0070 - acc: 0.4954 - val_loss: 0.9863 - val_acc: 0.4970\n",
      "Epoch 6/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 1.0052 - acc: 0.4912Epoch 00006: val_acc improved from 0.50298 to 0.54167, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-006-0.542\n",
      "197/197 [==============================] - 134s 679ms/step - loss: 1.0060 - acc: 0.4903 - val_loss: 0.9963 - val_acc: 0.5417\n",
      "Epoch 7/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9942 - acc: 0.4989Epoch 00007: val_acc did not improve\n",
      "197/197 [==============================] - 134s 681ms/step - loss: 0.9941 - acc: 0.4989 - val_loss: 1.0264 - val_acc: 0.4762\n",
      "Epoch 8/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9863 - acc: 0.5005Epoch 00008: val_acc did not improve\n",
      "197/197 [==============================] - 133s 677ms/step - loss: 0.9855 - acc: 0.5015 - val_loss: 0.9882 - val_acc: 0.5327\n",
      "Epoch 9/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9957 - acc: 0.5161Epoch 00009: val_acc did not improve\n",
      "197/197 [==============================] - 135s 687ms/step - loss: 0.9959 - acc: 0.5154 - val_loss: 0.9823 - val_acc: 0.5089\n",
      "Epoch 10/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9913 - acc: 0.5146Epoch 00010: val_acc did not improve\n",
      "197/197 [==============================] - 143s 724ms/step - loss: 0.9920 - acc: 0.5142 - val_loss: 0.9921 - val_acc: 0.5000\n",
      "Epoch 11/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9887 - acc: 0.5133Epoch 00011: val_acc did not improve\n",
      "197/197 [==============================] - 144s 729ms/step - loss: 0.9889 - acc: 0.5122 - val_loss: 1.0135 - val_acc: 0.4732\n",
      "Epoch 12/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9881 - acc: 0.5087Epoch 00012: val_acc did not improve\n",
      "197/197 [==============================] - 139s 704ms/step - loss: 0.9887 - acc: 0.5081 - val_loss: 1.0211 - val_acc: 0.5208\n",
      "Epoch 13/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9950 - acc: 0.5003Epoch 00013: val_acc did not improve\n",
      "197/197 [==============================] - 137s 696ms/step - loss: 0.9958 - acc: 0.4994 - val_loss: 0.9930 - val_acc: 0.5060\n",
      "Epoch 14/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9821 - acc: 0.5138Epoch 00014: val_acc did not improve\n",
      "197/197 [==============================] - 159s 806ms/step - loss: 0.9820 - acc: 0.5138 - val_loss: 0.9744 - val_acc: 0.5327\n",
      "Epoch 15/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9846 - acc: 0.5168Epoch 00015: val_acc did not improve\n",
      "197/197 [==============================] - 173s 879ms/step - loss: 0.9845 - acc: 0.5164 - val_loss: 1.0338 - val_acc: 0.4554\n",
      "Epoch 16/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9894 - acc: 0.5089Epoch 00016: val_acc did not improve\n",
      "197/197 [==============================] - 136s 689ms/step - loss: 0.9894 - acc: 0.5094 - val_loss: 0.9637 - val_acc: 0.5387\n",
      "Epoch 17/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9782 - acc: 0.5167Epoch 00017: val_acc did not improve\n",
      "197/197 [==============================] - 146s 741ms/step - loss: 0.9779 - acc: 0.5173 - val_loss: 1.0081 - val_acc: 0.4613\n",
      "Epoch 18/100\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9821 - acc: 0.5237Epoch 00018: val_acc did not improve\n",
      "197/197 [==============================] - 146s 742ms/step - loss: 0.9816 - acc: 0.5233 - val_loss: 0.9601 - val_acc: 0.5179\n",
      "Epoch 19/100\n",
      "  3/197 [..............................] - ETA: 1:19 - loss: 0.9637 - acc: 0.5625"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8f247fd51a19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_validation_samples\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                    )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1154\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m                                         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2075\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2076\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1794\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1796\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1797\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must compile your model before using it.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_inputs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_targets\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_sample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_check_trainable_weights_consistency\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         if (count_params(self.trainable_weights) !=\n\u001b[0;32m--> 971\u001b[0;31m                 count_params(self._collected_trainable_weights)):\n\u001b[0m\u001b[1;32m    972\u001b[0m             warnings.warn(UserWarning(\n\u001b[1;32m    973\u001b[0m                 \u001b[0;34m'Discrepancy between trainable weights and collected trainable'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36mcount_params\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mscalars\u001b[0m \u001b[0mcomposing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/utils/layer_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mThe\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0mnumber\u001b[0m \u001b[0mof\u001b[0m \u001b[0mscalars\u001b[0m \u001b[0mcomposing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36mcount_params\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    351\u001b[0m     \"\"\"\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# We don't want those compilation to show up in Theano profiler.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    324\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    327\u001b[0m     \u001b[0;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;31m# borrowed used defined inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1792\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1794\u001b[0;31m                    \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1795\u001b[0m             defaults)\n\u001b[1;32m   1796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[1;32m   1472\u001b[0m                         optimizer, inputs, outputs)\n\u001b[1;32m   1473\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1474\u001b[0;31m                     \u001b[0moptimizer_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    233\u001b[0m                 \u001b[0mnb_nodes_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0msub_prof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0msub_profs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph, start_from)\u001b[0m\n\u001b[1;32m   2381\u001b[0m                     self.cleanup_optimizers):\n\u001b[1;32m   2382\u001b[0m             \u001b[0mglobal_process_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m             \u001b[0mtime_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2384\u001b[0m             \u001b[0mnode_created\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, callbacks=[callback],\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/82\n",
      "196/197 [============================>.] - ETA: 1s - loss: 0.9731 - acc: 0.5311Epoch 00001: val_acc improved from -inf to 0.47917, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-001-0.479\n",
      "197/197 [==============================] - 225s 1s/step - loss: 0.9724 - acc: 0.5322 - val_loss: 0.9782 - val_acc: 0.4792\n",
      "Epoch 2/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9798 - acc: 0.5206Epoch 00002: val_acc did not improve\n",
      "197/197 [==============================] - 169s 859ms/step - loss: 0.9806 - acc: 0.5195 - val_loss: 0.9744 - val_acc: 0.4732\n",
      "Epoch 3/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9775 - acc: 0.5148Epoch 00003: val_acc did not improve\n",
      "197/197 [==============================] - 145s 735ms/step - loss: 0.9771 - acc: 0.5147 - val_loss: 1.0005 - val_acc: 0.4405\n",
      "Epoch 4/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9774 - acc: 0.5122Epoch 00004: val_acc improved from 0.47917 to 0.51786, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-004-0.518\n",
      "197/197 [==============================] - 169s 857ms/step - loss: 0.9775 - acc: 0.5122 - val_loss: 0.9391 - val_acc: 0.5179\n",
      "Epoch 5/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9743 - acc: 0.5114Epoch 00005: val_acc did not improve\n",
      "197/197 [==============================] - 162s 820ms/step - loss: 0.9745 - acc: 0.5107 - val_loss: 1.0012 - val_acc: 0.4821\n",
      "Epoch 6/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9727 - acc: 0.5158Epoch 00006: val_acc did not improve\n",
      "197/197 [==============================] - 159s 807ms/step - loss: 0.9730 - acc: 0.5157 - val_loss: 1.0141 - val_acc: 0.4732\n",
      "Epoch 7/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9654 - acc: 0.5177Epoch 00007: val_acc improved from 0.51786 to 0.54464, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-007-0.545\n",
      "197/197 [==============================] - 176s 893ms/step - loss: 0.9648 - acc: 0.5183 - val_loss: 0.9450 - val_acc: 0.5446\n",
      "Epoch 8/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9697 - acc: 0.5039Epoch 00008: val_acc did not improve\n",
      "197/197 [==============================] - 181s 917ms/step - loss: 0.9707 - acc: 0.5036 - val_loss: 0.9748 - val_acc: 0.4911\n",
      "Epoch 9/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9679 - acc: 0.5247Epoch 00009: val_acc did not improve\n",
      "197/197 [==============================] - 179s 910ms/step - loss: 0.9678 - acc: 0.5246 - val_loss: 1.0167 - val_acc: 0.5000\n",
      "Epoch 10/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9690 - acc: 0.5199Epoch 00010: val_acc did not improve\n",
      "197/197 [==============================] - 183s 926ms/step - loss: 0.9687 - acc: 0.5201 - val_loss: 0.9915 - val_acc: 0.4702\n",
      "Epoch 11/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9734 - acc: 0.5209Epoch 00011: val_acc did not improve\n",
      "197/197 [==============================] - 198s 1s/step - loss: 0.9730 - acc: 0.5214 - val_loss: 0.9791 - val_acc: 0.5089\n",
      "Epoch 12/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9771 - acc: 0.5103Epoch 00012: val_acc did not improve\n",
      "197/197 [==============================] - 208s 1s/step - loss: 0.9767 - acc: 0.5105 - val_loss: 0.9448 - val_acc: 0.5208\n",
      "Epoch 13/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9719 - acc: 0.5314Epoch 00013: val_acc did not improve\n",
      "197/197 [==============================] - 198s 1s/step - loss: 0.9727 - acc: 0.5306 - val_loss: 0.9912 - val_acc: 0.4970\n",
      "Epoch 14/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9655 - acc: 0.5207Epoch 00014: val_acc did not improve\n",
      "197/197 [==============================] - 209s 1s/step - loss: 0.9656 - acc: 0.5209 - val_loss: 0.9554 - val_acc: 0.5446\n",
      "Epoch 15/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9702 - acc: 0.5189Epoch 00015: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9722 - acc: 0.5175 - val_loss: 0.9714 - val_acc: 0.5119\n",
      "Epoch 16/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9810 - acc: 0.5202Epoch 00016: val_acc did not improve\n",
      "197/197 [==============================] - 206s 1s/step - loss: 0.9801 - acc: 0.5207 - val_loss: 0.9872 - val_acc: 0.4821\n",
      "Epoch 17/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9656 - acc: 0.5309Epoch 00017: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9659 - acc: 0.5313 - val_loss: 0.9750 - val_acc: 0.5179\n",
      "Epoch 18/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9710 - acc: 0.5254Epoch 00018: val_acc did not improve\n",
      "197/197 [==============================] - 202s 1s/step - loss: 0.9704 - acc: 0.5256 - val_loss: 0.9183 - val_acc: 0.5357\n",
      "Epoch 19/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9686 - acc: 0.5174Epoch 00019: val_acc did not improve\n",
      "197/197 [==============================] - 204s 1s/step - loss: 0.9682 - acc: 0.5176 - val_loss: 0.9841 - val_acc: 0.4821\n",
      "Epoch 20/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9617 - acc: 0.5340Epoch 00020: val_acc did not improve\n",
      "197/197 [==============================] - 202s 1s/step - loss: 0.9616 - acc: 0.5338 - val_loss: 0.9600 - val_acc: 0.5149\n",
      "Epoch 21/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9598 - acc: 0.5177Epoch 00021: val_acc did not improve\n",
      "197/197 [==============================] - 201s 1s/step - loss: 0.9597 - acc: 0.5186 - val_loss: 0.9889 - val_acc: 0.4673\n",
      "Epoch 22/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9705 - acc: 0.5142Epoch 00022: val_acc did not improve\n",
      "197/197 [==============================] - 197s 999ms/step - loss: 0.9702 - acc: 0.5138 - val_loss: 0.9483 - val_acc: 0.5060\n",
      "Epoch 23/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9627 - acc: 0.5295Epoch 00023: val_acc did not improve\n",
      "197/197 [==============================] - 198s 1s/step - loss: 0.9627 - acc: 0.5303 - val_loss: 0.9608 - val_acc: 0.5119\n",
      "Epoch 24/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9682 - acc: 0.5218Epoch 00024: val_acc did not improve\n",
      "197/197 [==============================] - 199s 1s/step - loss: 0.9680 - acc: 0.5217 - val_loss: 0.9435 - val_acc: 0.5298\n",
      "Epoch 25/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9609 - acc: 0.5277Epoch 00025: val_acc did not improve\n",
      "197/197 [==============================] - 198s 1s/step - loss: 0.9607 - acc: 0.5282 - val_loss: 0.9743 - val_acc: 0.4911\n",
      "Epoch 26/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9625 - acc: 0.5277Epoch 00026: val_acc did not improve\n",
      "197/197 [==============================] - 207s 1s/step - loss: 0.9617 - acc: 0.5285 - val_loss: 0.9558 - val_acc: 0.5238\n",
      "Epoch 27/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9546 - acc: 0.5378Epoch 00027: val_acc did not improve\n",
      "197/197 [==============================] - 199s 1s/step - loss: 0.9550 - acc: 0.5376 - val_loss: 0.9523 - val_acc: 0.5179\n",
      "Epoch 28/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9566 - acc: 0.5317Epoch 00028: val_acc did not improve\n",
      "197/197 [==============================] - 200s 1s/step - loss: 0.9576 - acc: 0.5309 - val_loss: 0.9687 - val_acc: 0.5089\n",
      "Epoch 29/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9645 - acc: 0.5286Epoch 00029: val_acc did not improve\n",
      "197/197 [==============================] - 197s 999ms/step - loss: 0.9639 - acc: 0.5294 - val_loss: 0.9795 - val_acc: 0.5089\n",
      "Epoch 30/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9467 - acc: 0.5388Epoch 00030: val_acc did not improve\n",
      "197/197 [==============================] - 200s 1s/step - loss: 0.9469 - acc: 0.5390 - val_loss: 0.9840 - val_acc: 0.5089\n",
      "Epoch 31/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9784 - acc: 0.5118Epoch 00031: val_acc did not improve\n",
      "197/197 [==============================] - 207s 1s/step - loss: 0.9782 - acc: 0.5124 - val_loss: 0.9794 - val_acc: 0.5089\n",
      "Epoch 32/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9457 - acc: 0.5395Epoch 00032: val_acc did not improve\n",
      "197/197 [==============================] - 208s 1s/step - loss: 0.9460 - acc: 0.5396 - val_loss: 0.9894 - val_acc: 0.5089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9514 - acc: 0.5368Epoch 00033: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9517 - acc: 0.5363 - val_loss: 1.0053 - val_acc: 0.5208\n",
      "Epoch 34/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9601 - acc: 0.5323Epoch 00034: val_acc did not improve\n",
      "197/197 [==============================] - 204s 1s/step - loss: 0.9602 - acc: 0.5321 - val_loss: 0.9388 - val_acc: 0.5417\n",
      "Epoch 35/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9567 - acc: 0.5328Epoch 00035: val_acc did not improve\n",
      "197/197 [==============================] - 202s 1s/step - loss: 0.9573 - acc: 0.5324 - val_loss: 0.9994 - val_acc: 0.5238\n",
      "Epoch 36/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9533 - acc: 0.5355Epoch 00036: val_acc improved from 0.54464 to 0.55060, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-036-0.551\n",
      "197/197 [==============================] - 200s 1s/step - loss: 0.9533 - acc: 0.5350 - val_loss: 0.9728 - val_acc: 0.5506\n",
      "Epoch 37/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9601 - acc: 0.5314Epoch 00037: val_acc did not improve\n",
      "197/197 [==============================] - 202s 1s/step - loss: 0.9596 - acc: 0.5325 - val_loss: 0.9761 - val_acc: 0.5208\n",
      "Epoch 38/82\n",
      "196/197 [============================>.] - ETA: 1s - loss: 0.9633 - acc: 0.5253Epoch 00038: val_acc did not improve\n",
      "197/197 [==============================] - 212s 1s/step - loss: 0.9639 - acc: 0.5245 - val_loss: 0.9592 - val_acc: 0.5298\n",
      "Epoch 39/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9517 - acc: 0.5284Epoch 00039: val_acc did not improve\n",
      "197/197 [==============================] - 209s 1s/step - loss: 0.9522 - acc: 0.5276 - val_loss: 0.9544 - val_acc: 0.5357\n",
      "Epoch 40/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9467 - acc: 0.5441Epoch 00040: val_acc did not improve\n",
      "197/197 [==============================] - 208s 1s/step - loss: 0.9462 - acc: 0.5445 - val_loss: 1.0015 - val_acc: 0.5000\n",
      "Epoch 41/82\n",
      "196/197 [============================>.] - ETA: 1s - loss: 0.9638 - acc: 0.5205Epoch 00041: val_acc did not improve\n",
      "197/197 [==============================] - 213s 1s/step - loss: 0.9633 - acc: 0.5211 - val_loss: 0.9942 - val_acc: 0.5089\n",
      "Epoch 42/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9418 - acc: 0.5493Epoch 00042: val_acc did not improve\n",
      "197/197 [==============================] - 209s 1s/step - loss: 0.9418 - acc: 0.5494 - val_loss: 0.9854 - val_acc: 0.5119\n",
      "Epoch 43/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9579 - acc: 0.5318Epoch 00043: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9582 - acc: 0.5323 - val_loss: 1.0090 - val_acc: 0.5179\n",
      "Epoch 44/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9452 - acc: 0.5393Epoch 00044: val_acc did not improve\n",
      "197/197 [==============================] - 207s 1s/step - loss: 0.9436 - acc: 0.5407 - val_loss: 1.0372 - val_acc: 0.4554\n",
      "Epoch 45/82\n",
      "196/197 [============================>.] - ETA: 1s - loss: 0.9558 - acc: 0.5274Epoch 00045: val_acc did not improve\n",
      "197/197 [==============================] - 211s 1s/step - loss: 0.9561 - acc: 0.5273 - val_loss: 0.9320 - val_acc: 0.5179\n",
      "Epoch 46/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9494 - acc: 0.5357Epoch 00046: val_acc did not improve\n",
      "197/197 [==============================] - 204s 1s/step - loss: 0.9499 - acc: 0.5355 - val_loss: 1.0126 - val_acc: 0.5179\n",
      "Epoch 47/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9571 - acc: 0.5334Epoch 00047: val_acc did not improve\n",
      "197/197 [==============================] - 209s 1s/step - loss: 0.9574 - acc: 0.5329 - val_loss: 0.9955 - val_acc: 0.5030\n",
      "Epoch 48/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9565 - acc: 0.5249Epoch 00048: val_acc did not improve\n",
      "197/197 [==============================] - 206s 1s/step - loss: 0.9563 - acc: 0.5244 - val_loss: 0.9390 - val_acc: 0.5268\n",
      "Epoch 49/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9498 - acc: 0.5389Epoch 00049: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9499 - acc: 0.5393 - val_loss: 0.9820 - val_acc: 0.5000\n",
      "Epoch 50/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9516 - acc: 0.5395Epoch 00050: val_acc did not improve\n",
      "197/197 [==============================] - 201s 1s/step - loss: 0.9516 - acc: 0.5393 - val_loss: 0.9970 - val_acc: 0.5387\n",
      "Epoch 51/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9450 - acc: 0.5465Epoch 00051: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9447 - acc: 0.5463 - val_loss: 0.9601 - val_acc: 0.5000\n",
      "Epoch 52/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9570 - acc: 0.5355Epoch 00052: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9562 - acc: 0.5366 - val_loss: 0.9785 - val_acc: 0.4762\n",
      "Epoch 53/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9527 - acc: 0.5298Epoch 00053: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9508 - acc: 0.5315 - val_loss: 0.9693 - val_acc: 0.5268\n",
      "Epoch 54/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9468 - acc: 0.5462Epoch 00054: val_acc did not improve\n",
      "197/197 [==============================] - 209s 1s/step - loss: 0.9479 - acc: 0.5456 - val_loss: 0.9959 - val_acc: 0.4821\n",
      "Epoch 55/82\n",
      "196/197 [============================>.] - ETA: 1s - loss: 0.9484 - acc: 0.5393Epoch 00055: val_acc did not improve\n",
      "197/197 [==============================] - 213s 1s/step - loss: 0.9487 - acc: 0.5395 - val_loss: 1.0267 - val_acc: 0.4583\n",
      "Epoch 56/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9632 - acc: 0.5266Epoch 00056: val_acc did not improve\n",
      "197/197 [==============================] - 211s 1s/step - loss: 0.9627 - acc: 0.5271 - val_loss: 0.9575 - val_acc: 0.5268\n",
      "Epoch 57/82\n",
      "196/197 [============================>.] - ETA: 1s - loss: 0.9445 - acc: 0.5397Epoch 00057: val_acc did not improve\n",
      "197/197 [==============================] - 212s 1s/step - loss: 0.9442 - acc: 0.5398 - val_loss: 0.9668 - val_acc: 0.4970\n",
      "Epoch 58/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9406 - acc: 0.5425Epoch 00058: val_acc did not improve\n",
      "197/197 [==============================] - 206s 1s/step - loss: 0.9414 - acc: 0.5426 - val_loss: 1.0057 - val_acc: 0.4911\n",
      "Epoch 59/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9507 - acc: 0.5376Epoch 00059: val_acc did not improve\n",
      "197/197 [==============================] - 204s 1s/step - loss: 0.9504 - acc: 0.5383 - val_loss: 0.9676 - val_acc: 0.4762\n",
      "Epoch 60/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9595 - acc: 0.5291Epoch 00060: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9592 - acc: 0.5287 - val_loss: 0.9480 - val_acc: 0.5149\n",
      "Epoch 61/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9456 - acc: 0.5392Epoch 00061: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9450 - acc: 0.5397 - val_loss: 1.0142 - val_acc: 0.5060\n",
      "Epoch 62/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9423 - acc: 0.5296Epoch 00062: val_acc did not improve\n",
      "197/197 [==============================] - 202s 1s/step - loss: 0.9422 - acc: 0.5304 - val_loss: 1.0214 - val_acc: 0.4762\n",
      "Epoch 63/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9440 - acc: 0.5534Epoch 00063: val_acc did not improve\n",
      "197/197 [==============================] - 202s 1s/step - loss: 0.9434 - acc: 0.5535 - val_loss: 0.9793 - val_acc: 0.4970\n",
      "Epoch 64/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9330 - acc: 0.5464Epoch 00064: val_acc did not improve\n",
      "197/197 [==============================] - 200s 1s/step - loss: 0.9326 - acc: 0.5468 - val_loss: 1.0000 - val_acc: 0.4792\n",
      "Epoch 65/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9402 - acc: 0.5499Epoch 00065: val_acc did not improve\n",
      "197/197 [==============================] - 210s 1s/step - loss: 0.9401 - acc: 0.5500 - val_loss: 0.9876 - val_acc: 0.4851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9532 - acc: 0.5312Epoch 00066: val_acc improved from 0.55060 to 0.55357, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-066-0.554\n",
      "197/197 [==============================] - 202s 1s/step - loss: 0.9530 - acc: 0.5308 - val_loss: 0.9585 - val_acc: 0.5536\n",
      "Epoch 67/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9548 - acc: 0.5487Epoch 00067: val_acc did not improve\n",
      "197/197 [==============================] - 195s 988ms/step - loss: 0.9547 - acc: 0.5485 - val_loss: 1.0032 - val_acc: 0.5119\n",
      "Epoch 68/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9477 - acc: 0.5341Epoch 00068: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9476 - acc: 0.5343 - val_loss: 0.9800 - val_acc: 0.5149\n",
      "Epoch 69/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9439 - acc: 0.5309Epoch 00069: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9434 - acc: 0.5317 - val_loss: 1.0266 - val_acc: 0.5179\n",
      "Epoch 70/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9472 - acc: 0.5548Epoch 00070: val_acc did not improve\n",
      "197/197 [==============================] - 199s 1s/step - loss: 0.9460 - acc: 0.5562 - val_loss: 0.9766 - val_acc: 0.5089\n",
      "Epoch 71/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9505 - acc: 0.5317Epoch 00071: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9519 - acc: 0.5306 - val_loss: 0.9843 - val_acc: 0.4881\n",
      "Epoch 72/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9483 - acc: 0.5416Epoch 00072: val_acc did not improve\n",
      "197/197 [==============================] - 206s 1s/step - loss: 0.9487 - acc: 0.5411 - val_loss: 0.9805 - val_acc: 0.5208\n",
      "Epoch 73/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9403 - acc: 0.5413Epoch 00073: val_acc did not improve\n",
      "197/197 [==============================] - 208s 1s/step - loss: 0.9405 - acc: 0.5411 - val_loss: 0.9908 - val_acc: 0.5060\n",
      "Epoch 74/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9561 - acc: 0.5372Epoch 00074: val_acc did not improve\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9557 - acc: 0.5379 - val_loss: 0.9505 - val_acc: 0.5268\n",
      "Epoch 75/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9412 - acc: 0.5360Epoch 00075: val_acc did not improve\n",
      "197/197 [==============================] - 206s 1s/step - loss: 0.9404 - acc: 0.5365 - val_loss: 1.0205 - val_acc: 0.5327\n",
      "Epoch 76/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9505 - acc: 0.5436Epoch 00076: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9501 - acc: 0.5440 - val_loss: 1.0142 - val_acc: 0.5000\n",
      "Epoch 77/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9452 - acc: 0.5409Epoch 00077: val_acc did not improve\n",
      "197/197 [==============================] - 206s 1s/step - loss: 0.9444 - acc: 0.5420 - val_loss: 0.9454 - val_acc: 0.5476\n",
      "Epoch 78/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9474 - acc: 0.5450Epoch 00078: val_acc did not improve\n",
      "197/197 [==============================] - 201s 1s/step - loss: 0.9474 - acc: 0.5454 - val_loss: 0.9997 - val_acc: 0.4792\n",
      "Epoch 79/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9276 - acc: 0.5459Epoch 00079: val_acc improved from 0.55357 to 0.59821, saving model to Workshop-Paper/Vanilla-Gaussian/vanila-gauss-079-0.598\n",
      "197/197 [==============================] - 203s 1s/step - loss: 0.9273 - acc: 0.5456 - val_loss: 0.9211 - val_acc: 0.5982\n",
      "Epoch 80/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9472 - acc: 0.5456Epoch 00080: val_acc did not improve\n",
      "197/197 [==============================] - 204s 1s/step - loss: 0.9469 - acc: 0.5451 - val_loss: 1.0235 - val_acc: 0.4554\n",
      "Epoch 81/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9500 - acc: 0.5334Epoch 00081: val_acc did not improve\n",
      "197/197 [==============================] - 206s 1s/step - loss: 0.9497 - acc: 0.5341 - val_loss: 0.9688 - val_acc: 0.4851\n",
      "Epoch 82/82\n",
      "196/197 [============================>.] - ETA: 0s - loss: 0.9423 - acc: 0.5457Epoch 00082: val_acc did not improve\n",
      "197/197 [==============================] - 205s 1s/step - loss: 0.9419 - acc: 0.5455 - val_loss: 0.9811 - val_acc: 0.5060\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, callbacks=[callback],\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs - 18,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 256, 256)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Workshop-Paper/Vanilla-Gaussian/vanila-gauss-079-0.598')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1979 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = ImageDataGenerator(rescale=1.0/100)\n",
    "test_data_stream = test_data_gen.flow_from_directory(\n",
    "                    'Data/Heatmaps/Train-Simple-Float-Gauss/ValidateFloatGaussian/',\n",
    "                    target_size=(256, 256),\n",
    "                    class_mode='categorical',\n",
    "                    shuffle=False,\n",
    "                    batch_size=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979/1979 [==============================] - 64s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(test_data_stream, 1979, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_classes = test_data_stream.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1979"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(actual_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1979"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanila Simple Gauss Validation Accuracy Score: 0.514906518444\n"
     ]
    }
   ],
   "source": [
    "print ('Vanila Simple Gauss Validation Accuracy Score:', accuracy_score(pred_classes, actual_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3157 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1.0/100)\n",
    "train_data_stream = train_data_gen.flow_from_directory(\n",
    "                    'Data/Heatmaps/Train-Simple-Float-Gauss/TrainFloatGaussian/',\n",
    "                    target_size=(256, 256),\n",
    "                    class_mode='categorical',\n",
    "                    shuffle=False,\n",
    "                    batch_size=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3157/3157 [==============================] - 96s 30ms/step\n",
      "Vanila Simple Gauss Train Accuracy Score: 0.567310738042\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict_generator(train_data_stream, 3157, verbose=1)\n",
    "actual_classes = train_data_stream.classes\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "print ('Vanila Simple Gauss Train Accuracy Score:', accuracy_score(pred_classes, actual_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Workshop-Paper/Vanilla-Gaussian/vanila-gauss-066-0.554')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1979/1979 [==============================] - 67s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.51490651844365842"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict_generator(test_data_stream, 1979, verbose=1)\n",
    "actual_classes = test_data_stream.classes\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "accuracy_score(actual_classes, pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training AlexNet on Raw Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.5\n",
    "N_CATEGORY = 3\n",
    "model_input = Input(shape = (3, 227, 227))\n",
    "\n",
    "# First convolutional Layer (96x11x11)\n",
    "z = Conv2D(filters = 96, kernel_size = (11,11), strides = (4,4), activation = \"relu\")(model_input)\n",
    "z = MaxPooling2D(pool_size = (3,3), strides=(2,2))(z)\n",
    "z = BatchNormalization()(z)\n",
    "\n",
    "# Second convolutional Layer (256x5x5)\n",
    "z = ZeroPadding2D(padding = (2,2))(z)\n",
    "z = Convolution2D(filters = 256, kernel_size = (5,5), strides = (1,1), activation = \"relu\")(z)\n",
    "z = MaxPooling2D(pool_size = (3,3), strides=(2,2))(z)\n",
    "z = BatchNormalization()(z)\n",
    "\n",
    "# Rest 3 convolutional layers\n",
    "z = ZeroPadding2D(padding = (1,1))(z)\n",
    "z = Convolution2D(filters = 384, kernel_size = (3,3), strides = (1,1), activation = \"relu\")(z)\n",
    "\n",
    "z = ZeroPadding2D(padding = (1,1))(z)\n",
    "z = Convolution2D(filters = 384, kernel_size = (3,3), strides = (1,1), activation = \"relu\")(z)\n",
    "\n",
    "z = ZeroPadding2D(padding = (1,1))(z)\n",
    "z = Convolution2D(filters = 256, kernel_size = (3,3), strides = (1,1), activation = \"relu\")(z)\n",
    "\n",
    "z = MaxPooling2D(pool_size = (3,3), strides=(2,2))(z)\n",
    "z = Flatten()(z)\n",
    "\n",
    "z = Dense(4096, activation=\"relu\")(z)\n",
    "z = Dropout(DROPOUT)(z)\n",
    "\n",
    "z = Dense(4096, activation=\"relu\")(z)\n",
    "z = Dropout(DROPOUT)(z)\n",
    "\n",
    "final_dim = 1 if N_CATEGORY == 2 else N_CATEGORY\n",
    "final_act = \"sigmoid\" if N_CATEGORY == 2 else \"softmax\"\n",
    "model_output = Dense(final_dim, activation=final_act)(z)\n",
    "\n",
    "model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.5\n",
    "ALPHA = 1e-4\n",
    "BETA = 0.75\n",
    "n = 5\n",
    "k = 2\n",
    "\n",
    "loss_metric = \"binary_crossentropy\" if N_CATEGORY == 2 else \"categorical_crossentropy\"\n",
    "model.compile(loss=loss_metric, metrics=[\"accuracy\"], \n",
    "              optimizer=optimizers.SGD(lr=LEARNING_RATE, momentum=MOMENTUM, decay = WEIGHT_DECAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "          rotation_range=40, width_shift_range=0.2,\n",
    "          height_shift_range=0.2, rescale=1./100,\n",
    "          shear_range=0.2, zoom_range=0.2,\n",
    "          horizontal_flip=True, fill_mode='nearest'\n",
    "           )\n",
    "\n",
    "validate_datagen = ImageDataGenerator(rescale=1./100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'Data/Train/'\n",
    "validation_data_dir = 'Data/Train-Val/'\n",
    "nb_train_samples = 1043 + 1079 + 1145\n",
    "nb_validation_samples = 116 + 120 + 127\n",
    "epochs = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3266 images belonging to 3 classes.\n",
      "Found 363 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    train_data_dir,\n",
    "                    target_size=(227, 227),\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode='categorical'\n",
    "                    )\n",
    "\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "                            validation_data_dir,\n",
    "                            target_size=(227, 227),\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode='categorical'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(filepath='Workshop-Paper/Alexnet-Raw/vanila-raw-{epoch:03d}-{val_acc:.3f}', save_best_only=True, monitor='val_acc', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "174/204 [========================>.....] - ETA: 23s - loss: 1.1443 - acc: 0.3373"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/PIL/TiffImagePlugin.py:742: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/204 [============================>.] - ETA: 0s - loss: 1.1382 - acc: 0.3384Epoch 00001: val_acc improved from -inf to 0.34659, saving model to Workshop-Paper/Alexnet-Raw/vanila-raw-001-0.347\n",
      "204/204 [==============================] - 170s 834ms/step - loss: 1.1381 - acc: 0.3379 - val_loss: 1.0995 - val_acc: 0.3466\n",
      "Epoch 2/100\n",
      "203/204 [============================>.] - ETA: 0s - loss: 1.0984 - acc: 0.3436Epoch 00002: val_acc did not improve\n",
      "204/204 [==============================] - 161s 791ms/step - loss: 1.0982 - acc: 0.3437 - val_loss: 1.1084 - val_acc: 0.3182\n",
      "Epoch 3/100\n",
      "203/204 [============================>.] - ETA: 0s - loss: 1.1011 - acc: 0.3630Epoch 00003: val_acc improved from 0.34659 to 0.39489, saving model to Workshop-Paper/Alexnet-Raw/vanila-raw-003-0.395\n",
      "204/204 [==============================] - 175s 858ms/step - loss: 1.1012 - acc: 0.3624 - val_loss: 1.0872 - val_acc: 0.3949\n",
      "Epoch 4/100\n",
      "203/204 [============================>.] - ETA: 0s - loss: 1.0966 - acc: 0.3833Epoch 00004: val_acc did not improve\n",
      "204/204 [==============================] - 164s 802ms/step - loss: 1.0967 - acc: 0.3823 - val_loss: 1.0975 - val_acc: 0.3466\n",
      "Epoch 5/100\n",
      "203/204 [============================>.] - ETA: 0s - loss: 1.0965 - acc: 0.3608Epoch 00005: val_acc did not improve\n",
      "204/204 [==============================] - 157s 770ms/step - loss: 1.0966 - acc: 0.3609 - val_loss: 1.0976 - val_acc: 0.3551\n",
      "Epoch 6/100\n",
      "203/204 [============================>.] - ETA: 0s - loss: 1.0932 - acc: 0.3778Epoch 00006: val_acc improved from 0.39489 to 0.42614, saving model to Workshop-Paper/Alexnet-Raw/vanila-raw-006-0.426\n",
      "204/204 [==============================] - 158s 773ms/step - loss: 1.0931 - acc: 0.3784 - val_loss: 1.0836 - val_acc: 0.4261\n",
      "Epoch 7/100\n",
      "203/204 [============================>.] - ETA: 1s - loss: 1.0935 - acc: 0.3722Epoch 00007: val_acc did not improve\n",
      "204/204 [==============================] - 224s 1s/step - loss: 1.0935 - acc: 0.3722 - val_loss: 1.0989 - val_acc: 0.3210\n",
      "Epoch 8/100\n",
      "103/204 [==============>...............] - ETA: 1:11 - loss: 1.0924 - acc: 0.3659"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, callbacks=[callback],\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
