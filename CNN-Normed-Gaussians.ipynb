{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, MaxPooling2D, Convolution2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, Input, BatchNormalization, ZeroPadding2D\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "          rotation_range=40, width_shift_range=0.2,\n",
    "          height_shift_range=0.2, rescale=1./100,\n",
    "          shear_range=0.2, zoom_range=0.2,\n",
    "          horizontal_flip=True, fill_mode='nearest'\n",
    "           )\n",
    "\n",
    "validate_datagen = ImageDataGenerator(rescale=1./100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'Data/Heatmaps/Train-Normed-Float-Gauss/TrainGaussianNormalizedCenter/'\n",
    "validation_data_dir = 'Data/Heatmaps/Train-Normed-Float-Gauss/TrainGaussianNormalizedCenter-Val/'\n",
    "nb_train_samples = 1117 + 1150 + 1241\n",
    "nb_validation_samples = 112 + 115 + 124\n",
    "epochs = 100\n",
    "batch_size = 16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 256, 256)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    train_data_dir,\n",
    "                    target_size=(256, 256),\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode='categorical'\n",
    "                    )\n",
    "\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "                            validation_data_dir,\n",
    "                            target_size=(256, 256),\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode='categorical'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(filepath='Workshop-Paper/Vanilla-Normed/vanila-normed-{epoch:03d}-{val_acc:.3f}', save_best_only=True, monitor='val_acc', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, callbacks=[callback],\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(3, 256, 256)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('Workshop-Paper/Vanilla-Normed/vanila-normed-063-0.601')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_gen = ImageDataGenerator(rescale=1.0/100)\n",
    "test_data_stream = test_data_gen.flow_from_directory(\n",
    "                    'Data/Heatmaps/Train-Normed-Float-Gauss/ValidateGaussianNormalizedCenter/',\n",
    "                    target_size=(256, 256),\n",
    "                    class_mode='categorical',\n",
    "                    shuffle=False,\n",
    "                    batch_size=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(test_data_stream, 1979, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_classes = test_data_stream.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(actual_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Vanila Normed Validation Accuracy Score:', accuracy_score(pred_classes, actual_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_gen = ImageDataGenerator(rescale=1.0/100)\n",
    "train_data_stream = train_data_gen.flow_from_directory(\n",
    "                    'Data/Heatmaps/Train-Normed-Float-Gauss/TrainGaussianNormalizedCenter/',\n",
    "                    target_size=(256, 256),\n",
    "                    class_mode='categorical',\n",
    "                    shuffle=False,\n",
    "                    batch_size=1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_generator(train_data_stream, 3157, verbose=1)\n",
    "actual_classes = train_data_stream.classes\n",
    "pred_classes = np.argmax(predictions, axis=1)\n",
    "print ('Vanila Normed Training Accuracy Score:', accuracy_score(pred_classes, actual_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training AlexNet on Raw Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT = 0.5\n",
    "N_CATEGORY = 3\n",
    "model_input = Input(shape = (3, 227, 227))\n",
    "\n",
    "# First convolutional Layer (96x11x11)\n",
    "z = Conv2D(filters = 96, kernel_size = (11,11), strides = (4,4), activation = \"relu\")(model_input)\n",
    "z = MaxPooling2D(pool_size = (3,3), strides=(2,2))(z)\n",
    "z = BatchNormalization()(z)\n",
    "\n",
    "# Second convolutional Layer (256x5x5)\n",
    "z = ZeroPadding2D(padding = (2,2))(z)\n",
    "z = Convolution2D(filters = 256, kernel_size = (5,5), strides = (1,1), activation = \"relu\")(z)\n",
    "z = MaxPooling2D(pool_size = (3,3), strides=(2,2))(z)\n",
    "z = BatchNormalization()(z)\n",
    "\n",
    "# Rest 3 convolutional layers\n",
    "z = ZeroPadding2D(padding = (1,1))(z)\n",
    "z = Convolution2D(filters = 384, kernel_size = (3,3), strides = (1,1), activation = \"relu\")(z)\n",
    "\n",
    "z = ZeroPadding2D(padding = (1,1))(z)\n",
    "z = Convolution2D(filters = 384, kernel_size = (3,3), strides = (1,1), activation = \"relu\")(z)\n",
    "\n",
    "z = ZeroPadding2D(padding = (1,1))(z)\n",
    "z = Convolution2D(filters = 256, kernel_size = (3,3), strides = (1,1), activation = \"relu\")(z)\n",
    "\n",
    "z = MaxPooling2D(pool_size = (3,3), strides=(2,2))(z)\n",
    "z = Flatten()(z)\n",
    "\n",
    "z = Dense(4096, activation=\"relu\")(z)\n",
    "z = Dropout(DROPOUT)(z)\n",
    "\n",
    "z = Dense(4096, activation=\"relu\")(z)\n",
    "z = Dropout(DROPOUT)(z)\n",
    "\n",
    "final_dim = 1 if N_CATEGORY == 2 else N_CATEGORY\n",
    "final_act = \"sigmoid\" if N_CATEGORY == 2 else \"softmax\"\n",
    "model_output = Dense(final_dim, activation=final_act)(z)\n",
    "\n",
    "model = Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHT_DECAY = 0.0005\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.01\n",
    "DROPOUT = 0.5\n",
    "ALPHA = 1e-4\n",
    "BETA = 0.75\n",
    "n = 5\n",
    "k = 2\n",
    "\n",
    "loss_metric = \"binary_crossentropy\" if N_CATEGORY == 2 else \"categorical_crossentropy\"\n",
    "model.compile(loss=loss_metric, metrics=[\"accuracy\"], \n",
    "              optimizer=optimizers.SGD(lr=LEARNING_RATE, momentum=MOMENTUM, decay = WEIGHT_DECAY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "          rotation_range=40, width_shift_range=0.2,\n",
    "          height_shift_range=0.2, rescale=1./100,\n",
    "          shear_range=0.2, zoom_range=0.2,\n",
    "          horizontal_flip=True, fill_mode='nearest'\n",
    "           )\n",
    "\n",
    "validate_datagen = ImageDataGenerator(rescale=1./100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = 'Data/Train/'\n",
    "validation_data_dir = 'Data/Train-Val/'\n",
    "nb_train_samples = 1043 + 1079 + 1145\n",
    "nb_validation_samples = 116 + 120 + 127\n",
    "epochs = 100\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    train_data_dir,\n",
    "                    target_size=(227, 227),\n",
    "                    batch_size=batch_size,\n",
    "                    class_mode='categorical'\n",
    "                    )\n",
    "\n",
    "validation_generator = validate_datagen.flow_from_directory(\n",
    "                            validation_data_dir,\n",
    "                            target_size=(227, 227),\n",
    "                            batch_size=batch_size,\n",
    "                            class_mode='categorical'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint(filepath='Workshop-Paper/Alexnet-Raw/vanila-raw-{epoch:03d}-{val_acc:.3f}', save_best_only=True, monitor='val_acc', verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit_generator(train_generator, callbacks=[callback],\n",
    "                    steps_per_epoch=nb_train_samples // batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=nb_validation_samples // batch_size\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
